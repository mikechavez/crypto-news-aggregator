# Narratives Endpoint Optimization Summary

## Changes Made to `/api/v1/narratives/active`

### 1. **Replaced Sequential Queries with Single Aggregation Pipeline**

**Before:**
- Called `get_active_narratives()` which executed a simple `find()` query
- Fetched all fields including heavy ones (fingerprint vectors, lifecycle_history, timeline_data)
- Post-processed data in Python loops

**After:**
- Single MongoDB aggregation pipeline with 4 stages:
  1. **$match**: Filter by lifecycle_state (emerging, rising, hot, cooling, reactivated)
  2. **$sort**: Sort by last_updated descending
  3. **$limit**: Limit to requested number (default 50)
  4. **$project**: Exclude heavy fields (fingerprint, lifecycle_history, timeline_data)

```python
pipeline = [
    {'$match': match_stage},
    {'$sort': {'last_updated': -1}},
    {'$limit': limit},
    {'$project': {
        # Include only necessary fields
        '_id': 1, 'theme': 1, 'title': 1, 'summary': 1, 'entities': 1,
        'article_count': 1, 'mention_velocity': 1, 'lifecycle': 1,
        'lifecycle_state': 1, 'momentum': 1, 'recency_score': 1,
        'entity_relationships': 1, 'first_seen': 1, 'last_updated': 1,
        'days_active': 1, 'peak_activity': 1, 'reawakening_count': 1,
        'reawakened_from': 1, 'resurrection_velocity': 1,
        # Exclude heavy fields
        'fingerprint': 0, 'lifecycle_history': 0, 'timeline_data': 0
    }}
]
```

### 2. **Added In-Memory Caching with 1-Minute TTL**

**Before:**
- Used Redis caching with 10-minute TTL
- Fallback to no caching if Redis unavailable

**After:**
- In-memory Python dictionary cache with 1-minute TTL
- Similar pattern to signals endpoint
- Faster cache hits (no network overhead)
- More aggressive TTL for fresher data

```python
# Cache structure
_narratives_cache: Dict[str, tuple[Any, datetime]] = {}
_narratives_cache_ttl = timedelta(minutes=1)

# Cache check
if cache_key in _narratives_cache:
    cached_data, cached_time = _narratives_cache[cache_key]
    if datetime.now() - cached_time < _narratives_cache_ttl:
        return cached_data
```

### 3. **Added lifecycle_state Filter Parameter**

New query parameter allows filtering by specific lifecycle states:
- `?lifecycle_state=emerging`
- `?lifecycle_state=hot`
- `?lifecycle_state=mature`

### 4. **Excluded Heavy Fields from List View**

**Excluded fields (set to None in response):**
- `fingerprint`: Large vector arrays (hundreds of floats)
- `lifecycle_history`: Historical state transitions
- `timeline_data`: Daily snapshots

These fields are still available in the detail view (`GET /narratives/{id}`)

## Performance Improvements

### Expected Results:
- **Cache MISS**: ~200-400ms (down from 1-3 seconds)
  - Single aggregation pipeline vs multiple queries
  - Projection reduces data transfer
  
- **Cache HIT**: ~1-5ms
  - In-memory lookup (no database or Redis network call)
  - 200-400x faster than cache miss

### Data Transfer Reduction:
- Fingerprint vectors: ~1-2KB per narrative
- Lifecycle history: ~500B-1KB per narrative
- Timeline data: ~2-5KB per narrative
- **Total savings**: ~3-8KB per narrative Ã— 50 narratives = **150-400KB less data**

## Testing

To test the optimized endpoint:

```bash
# Test cache miss (first request)
time curl "http://localhost:8000/api/v1/narratives/active?limit=50"

# Test cache hit (immediate second request)
time curl "http://localhost:8000/api/v1/narratives/active?limit=50"

# Test with lifecycle_state filter
time curl "http://localhost:8000/api/v1/narratives/active?lifecycle_state=hot&limit=20"
```

## Backward Compatibility

- All existing fields still present in response
- Heavy fields set to `null` instead of omitted
- Frontend can still request full details via `GET /narratives/{id}`
- Old `story` and `updated_at` fields maintained for compatibility
